{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vojte\\AppData\\Local\\Temp\\ipykernel_17640\\1015519785.py:1: DtypeWarning: Columns (65,76,91,154,170,171) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"train_data.csv\", index_col=0) # Load the training data\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train_data.csv\", index_col=0) # Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "examined_columns = [\"FTR\", # Dependent variable\n",
    "                    \"Attendance\", \"HS\", \"AS\", \"HST\", \"AST\", \"HHW\", \"AHW\", \"HC\", \"AC\", \n",
    "                    \"HF\", \"AF\", \"HFKC\", \"AFKC\", \"HO\", \"AO\", \"HY\", \"AY\", \"HR\", \"AR\", \"HBP\", \"ABP\"]\n",
    "X = df[examined_columns].copy()\n",
    "if X[\"FTR\"].dtype != np.float64:\n",
    "    X[\"FTR\"] = X[\"FTR\"].map(\n",
    "        {\"D\": 0, \"A\": -1, \"H\": 1}\n",
    "    ) # Map categorical variables to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AFKC          153867\n",
       "HFKC          153867\n",
       "AO            148674\n",
       "HHW           148666\n",
       "AHW           148654\n",
       "HO            148653\n",
       "ABP           148502\n",
       "Attendance    148497\n",
       "HBP           148486\n",
       "dtype: int64"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().sum().sort_values(ascending=False)[:9]\n",
    "# Some variables have a lot of missing values\n",
    "# Our data has 153k entries, let's drop these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HF     65013\n",
       "AF     65004\n",
       "HST    63882\n",
       "AST    63855\n",
       "HC     63377\n",
       "dtype: int64"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop(columns=[\"HFKC\", \"AFKC\", \"AO\", \"HO\", \"HHW\", \"AHW\", \"HBP\", \"ABP\", \"Attendance\"], inplace = True)\n",
    "X.isna().sum().sort_values(ascending=False)[:5]\n",
    "# Dropping some columns now will not remove most of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(inplace=True)\n",
    "y = X.pop(\"FTR\") # Remove the dependent variable from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS: 0.016418610620113627\n",
      "AS: 0.016391956573126087\n",
      "HST: 0.061069021464353\n",
      "AST: 0.06120874609405291\n",
      "HC: 0.00233621778902271\n",
      "AC: 0.002256350139400398\n",
      "HF: 0.0\n",
      "AF: 0.002287879626687861\n",
      "HY: 0.0072448686291139985\n",
      "AY: 0.0024203048760282897\n",
      "HR: 0.007968527624721888\n",
      "AR: 0.006269654091008325\n"
     ]
    }
   ],
   "source": [
    "mutual_info_scores = mutual_info_classif(X, y)\n",
    "for feature, score in zip(X.columns, mutual_info_scores):\n",
    "    print(f\"{feature}: {score}\")\n",
    "# We see that some variables have a very negligible effect on the dependent variable\n",
    "# We can drop those as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between HS and target is 0.17\n",
      "Correlation between AS and target is -0.17\n",
      "Correlation between HST and target is 0.31\n",
      "Correlation between AST and target is -0.31\n",
      "Correlation between HC and target is -0.02\n",
      "Correlation between AC and target is 0.0\n",
      "Correlation between HF and target is -0.0\n",
      "Correlation between AF and target is 0.0\n",
      "Correlation between HY and target is -0.08\n",
      "Correlation between AY and target is 0.03\n",
      "Correlation between HR and target is -0.13\n",
      "Correlation between AR and target is 0.12\n"
     ]
    }
   ],
   "source": [
    "for feature in X.columns:\n",
    "    print(f\"Correlation between {feature} and target is {round(np.corrcoef(X[feature], y)[1][0], 2)}\")\n",
    "# Correlation analysis tells us, that there are some weakly colinear variables with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88813, 12)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "# In this analysis, we have worked with roughly 88k entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2492031165585853"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "model.score(X, y)\n",
    "# Here we check the importance of the variables using a linear regression\n",
    "# The R^2 is around 0.25, which means that we should choose more variables to include in future models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FTHG',\n",
       " 'Div',\n",
       " 'FTAG',\n",
       " 'HTAG',\n",
       " 'HTHG',\n",
       " 'Date',\n",
       " 'HTR',\n",
       " 'HomeTeam',\n",
       " 'AwayTeam',\n",
       " 'IWA',\n",
       " 'IWD',\n",
       " 'IWH',\n",
       " 'WHD',\n",
       " 'WHA',\n",
       " 'WHH',\n",
       " 'B365A',\n",
       " 'B365H',\n",
       " 'B365D']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict(df.isna().sum().sort_values()).keys())[4:22]\n",
    "# Some more candidate columns which don't have a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_columns = [\"FTHG\", \"FTAG\", \"HTAG\", \"HTHG\", \"IWA\", \"IWD\", \"IWH\", \"WHD\", \"WHA\", \"WHH\", \"B365A\", \"B365H\", \"B365D\"]\n",
    "# Selecting columns from the previous cells\n",
    "if df[\"FTR\"].dtype != np.float64:\n",
    "    df[\"FTR\"] = df[\"FTR\"].map(\n",
    "        {\"D\": 0, \"A\": -1, \"H\": 1}\n",
    "    ) # Map categorical variables to numerical\n",
    "X = df[examined_columns + more_columns].copy() # Add previously used columns\n",
    "X = X.drop(columns=[\"HFKC\", \"AFKC\", \"AO\", \"HO\", \"HHW\", \"AHW\", \"HBP\", \"ABP\", \"Attendance\"]).copy() # Drop categorical variables\n",
    "X.dropna(inplace=True)\n",
    "y = X.pop(\"FTR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5901337834862573"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "model.score(X, y)\n",
    "# This model's R^2 is 0.59, which is owing to more parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR',\n",
       "       'AR', 'FTHG', 'FTAG', 'HTAG', 'HTHG', 'IWA', 'IWD', 'IWH', 'WHD', 'WHA',\n",
       "       'WHH', 'B365A', 'B365H', 'B365D'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns\n",
    "# These are the final used variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
